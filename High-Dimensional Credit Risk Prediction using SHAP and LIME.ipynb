{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee980c4-15fc-469b-b707-8393933f6721",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix, precision_recall_fscore_support\n",
    "from xgboost import XGBClassifier\n",
    "import shap\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2) Load dataset \n",
    "DATA_PATH = \"/mnt/data/credit_risk_dataset.csv\"   \n",
    "assert os.path.exists(DATA_PATH), f\"File not found: {DATA_PATH}\"\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded dataset shape:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "# 3) Basic preprocessing: create derived features, drop ID, target separation\n",
    "df['earliest_cr_line'] = pd.to_datetime(df['earliest_cr_line'])\n",
    "df['credit_age_years'] = (pd.Timestamp.today() - df['earliest_cr_line']).dt.days / 365.25\n",
    "\n",
    "# Drop loan_id (unique id) and earliest_cr_line (we derived credit_age_years)\n",
    "if 'loan_id' in df.columns:\n",
    "    df = df.drop(columns=['loan_id'])\n",
    "if 'earliest_cr_line' in df.columns:\n",
    "    df = df.drop(columns=['earliest_cr_line'])\n",
    "\n",
    "# Target\n",
    "TARGET = 'target_default'\n",
    "assert TARGET in df.columns, f\"Target column '{TARGET}' not found.\"\n",
    "\n",
    "# Quick check: convert boolean-like columns if any\n",
    "# (none expected, but keep safe)\n",
    "for c in df.select_dtypes(include=['bool']).columns:\n",
    "    df[c] = df[c].astype(int)\n",
    "\n",
    "# 4) Train/test split\n",
    "X = df.drop(columns=[TARGET])\n",
    "y = df[TARGET].astype(int)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    test_size=0.20,\n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape, \"Default rate:\", y.mean())\n",
    "\n",
    "# 5) Build preprocessing pipeline\n",
    "num_cols = X.select_dtypes(include=['int64','float64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object','category']).columns.tolist()\n",
    "\n",
    "print(\"Numeric cols:\", num_cols)\n",
    "print(\"Categorical cols:\", cat_cols)\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "cat_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', num_pipeline, num_cols),\n",
    "    ('cat', cat_pipeline, cat_cols)\n",
    "], remainder='drop')\n",
    "\n",
    "# 6) Full pipeline with XGBoost classifier\n",
    "xgb = XGBClassifier(n_estimators=200, learning_rate=0.05, max_depth=6,\n",
    "                    use_label_encoder=False, eval_metric='logloss',\n",
    "                    random_state=42, n_jobs= -1)\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preproc', preprocessor),\n",
    "    ('clf', xgb)\n",
    "])\n",
    "\n",
    "# 7) Fit the pipeline (quick baseline)\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Baseline training complete.\")\n",
    "\n",
    "# 8) Predictions and evaluation\n",
    "y_pred = pipeline.predict(X_test)\n",
    "y_proba = pipeline.predict_proba(X_test)[:,1]\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "print(f\"AUC on test set: {auc:.4f}\")\n",
    "print(\"Classification report:\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion matrix:\\n\", cm)\n",
    "\n",
    "# Save model\n",
    "MODEL_PATH = \"/mnt/data/xgb_pipeline.joblib\"\n",
    "joblib.dump(pipeline, MODEL_PATH)\n",
    "print(\"Saved pipeline to:\", MODEL_PATH)\n",
    "\n",
    "# 9) Extract preprocessor output feature names (works with sklearn >=1.0)\n",
    "# We'll fit a small helper to get feature names after ColumnTransformer\n",
    "def get_feature_names(column_transformer):\n",
    "    \"\"\"\n",
    "    Return feature names from all transformers.\n",
    "    Expects the ColumnTransformer to be fitted.\n",
    "    \"\"\"\n",
    "    output_features = []\n",
    "    for name, transformer, columns in column_transformer.transformers_:\n",
    "        if name == 'remainder' and transformer == 'drop':\n",
    "            continue\n",
    "        if hasattr(transformer, 'named_steps') and 'ohe' in transformer.named_steps:\n",
    "            # OneHotEncoder inside pipeline\n",
    "            ohe = transformer.named_steps['ohe']\n",
    "            input_cols = columns\n",
    "            ohe_cols = list(ohe.get_feature_names_out(input_cols))\n",
    "            output_features.extend(ohe_cols)\n",
    "        elif isinstance(transformer, OneHotEncoder):\n",
    "            ohe = transformer\n",
    "            input_cols = columns\n",
    "            ohe_cols = list(ohe.get_feature_names_out(input_cols))\n",
    "            output_features.extend(ohe_cols)\n",
    "        else:\n",
    "            # numeric or other pipeline -> pass-through column names (maybe scaled)\n",
    "            if isinstance(columns, (list, np.ndarray)):\n",
    "                output_features.extend(list(columns))\n",
    "            else:\n",
    "                # if columns is slice / str\n",
    "                output_features.append(columns)\n",
    "    return output_features\n",
    "\n",
    "# fit preprocessor on training data to ensure transformers have been fit\n",
    "preprocessor_fitted = pipeline.named_steps['preproc']\n",
    "# If pipeline preproc is already fitted during pipeline.fit, transformers_ exist\n",
    "feature_names = get_feature_names(preprocessor_fitted)\n",
    "print(\"Number of features after preprocessing:\", len(feature_names))\n",
    "# convert X_test transformed to array for use in SHAP/LIME\n",
    "X_test_preprocessed = preprocessor_fitted.transform(X_test)\n",
    "if hasattr(X_test_preprocessed, \"toarray\"):\n",
    "    X_test_preprocessed = X_test_preprocessed.toarray()\n",
    "\n",
    "# 10) SHAP: global + local explanations (TreeExplainer for XGBoost)\n",
    "print(\"Computing SHAP values (this may take a minute)...\")\n",
    "model_for_shap = pipeline.named_steps['clf']\n",
    "explainer = shap.TreeExplainer(model_for_shap)\n",
    "# Use a sample for shap computations to save time\n",
    "sample_idx = np.random.RandomState(42).choice(X_test_preprocessed.shape[0], size=min(2000, X_test_preprocessed.shape[0]), replace=False)\n",
    "X_shap_sample = X_test_preprocessed[sample_idx]\n",
    "shap_values = explainer.shap_values(X_shap_sample)\n",
    "\n",
    "# SHAP summary plot (global)\n",
    "plt.figure(figsize=(10,6))\n",
    "shap.summary_plot(shap_values, X_shap_sample, feature_names=feature_names, show=False)\n",
    "plt.tight_layout()\n",
    "SHAP_SUMMARY_PNG = \"/mnt/data/shap_summary.png\"\n",
    "plt.savefig(SHAP_SUMMARY_PNG, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(\"Saved SHAP summary plot to:\", SHAP_SUMMARY_PNG)\n",
    "\n",
    "# 11) Save SHAP values for the sample to CSV (top contributions)\n",
    "# For convenience, compute mean absolute shap per feature (global importance)\n",
    "mean_abs_shap = np.abs(shap_values).mean(axis=0)\n",
    "shap_df = pd.DataFrame({'feature': feature_names, 'mean_abs_shap': mean_abs_shap})\n",
    "shap_df = shap_df.sort_values('mean_abs_shap', ascending=False)\n",
    "SHAP_CSV = \"/mnt/data/shap_feature_importance.csv\"\n",
    "shap_df.to_csv(SHAP_CSV, index=False)\n",
    "print(\"Saved SHAP importances to:\", SHAP_CSV)\n",
    "\n",
    "# 12) LIME: local explanations for raw features\n",
    "# We'll create a wrapper classifier that accepts raw X (pandas row(s)) and returns predict_proba\n",
    "def predict_proba_raw(raw_X):\n",
    "    \"\"\"\n",
    "    raw_X: 2D numpy array or list of raw feature rows in the original (pre-split) column order.\n",
    "    Returns: predict_proba of the pipeline for class probabilities.\n",
    "    \"\"\"\n",
    "    if isinstance(raw_X, np.ndarray):\n",
    "        arr = raw_X\n",
    "    else:\n",
    "        arr = np.array(raw_X)\n",
    "    # If arr is 1D (single instance), reshape\n",
    "    if arr.ndim == 1:\n",
    "        arr = arr.reshape(1, -1)\n",
    "    # Build DataFrame with same columns as X_train\n",
    "    df_raw = pd.DataFrame(arr, columns=X_train.columns)\n",
    "    proba = pipeline.predict_proba(df_raw)\n",
    "    return proba\n",
    "\n",
    "# Instantiate Lime explainer using training data in raw (original) feature space\n",
    "X_train_raw = X_train.reset_index(drop=True)\n",
    "lime_explainer = LimeTabularExplainer(X_train_raw.values,\n",
    "                                      feature_names=X_train_raw.columns.tolist(),\n",
    "                                      class_names=['no_default', 'default'],\n",
    "                                      discretize_continuous=True,\n",
    "                                      random_state=42)\n",
    "\n",
    "# 13) Choose three representative cases: clear approval, clear denial, borderline\n",
    "# We'll pick based on predicted probability extremes and near 0.5\n",
    "\n",
    "test_df = X_test.reset_index(drop=True)\n",
    "test_proba = pipeline.predict_proba(test_df)[:,1]\n",
    "test_pred = pipeline.predict(test_df)\n",
    "\n",
    "test_results = test_df.copy()\n",
    "test_results['proba_default'] = test_proba\n",
    "test_results['pred'] = test_pred\n",
    "test_results['true'] = y_test.reset_index(drop=True)\n",
    "\n",
    "# Clear approval: very low predicted prob (<=0.05) and predicted 0\n",
    "approval_candidates = test_results[test_results['proba_default'] <= 0.05]\n",
    "approval_idx = approval_candidates.index[0] if len(approval_candidates) > 0 else test_results['proba_default'].idxmin()\n",
    "\n",
    "# Clear denial: very high predicted prob (>=0.95) and predicted 1\n",
    "denial_candidates = test_results[test_results['proba_default'] >= 0.95]\n",
    "denial_idx = denial_candidates.index[0] if len(denial_candidates) > 0 else test_results['proba_default'].idxmax()\n",
    "\n",
    "# Borderline: nearest to 0.5\n",
    "borderline_idx = (test_results['proba_default'] - 0.5).abs().idxmin()\n",
    "\n",
    "selected_indices = [int(approval_idx), int(denial_idx), int(borderline_idx)]\n",
    "selected_indices, test_results.loc[selected_indices, ['proba_default','pred','true']]\n",
    "\n",
    "# 14) For each selected case compute SHAP local force values and LIME explanation and save results\n",
    "CASE_DIR = \"/mnt/data/case_explanations\"\n",
    "os.makedirs(CASE_DIR, exist_ok=True)\n",
    "\n",
    "for idx in selected_indices:\n",
    "    raw_row = test_df.iloc[idx:idx+1]  # keep as DataFrame\n",
    "    proba = float(test_results.loc[idx, 'proba_default'])\n",
    "    pred = int(test_results.loc[idx, 'pred'])\n",
    "    true = int(test_results.loc[idx, 'true'])\n",
    "    print(f\"\\nCase index {idx}  proba_default={proba:.4f} pred={pred} true={true}\")\n",
    "    # SHAP local explanation\n",
    "    # Need preprocessed version for shap\n",
    "    x_pre = preprocessor_fitted.transform(raw_row)\n",
    "    if hasattr(x_pre, \"toarray\"):\n",
    "        x_pre = x_pre.toarray()\n",
    "    shap_local_vals = explainer.shap_values(x_pre)\n",
    "    # create a DataFrame with shap contributions\n",
    "    shap_local_df = pd.DataFrame({'feature': feature_names,\n",
    "                                  'shap_value': shap_local_vals.ravel(),\n",
    "                                  'abs_shap': np.abs(shap_local_vals).ravel()})\n",
    "    shap_local_df = shap_local_df.sort_values('abs_shap', ascending=False)\n",
    "    shap_local_csv = os.path.join(CASE_DIR, f\"case_{idx}_shap_local.csv\")\n",
    "    shap_local_df.to_csv(shap_local_csv, index=False)\n",
    "    print(\"Saved SHAP local CSV for case\", idx, \"->\", shap_local_csv)\n",
    "\n",
    "    # Create a bar plot of top 10 shap contributions\n",
    "    plt.figure(figsize=(8,4))\n",
    "    topn = shap_local_df.head(10).sort_values('shap_value')\n",
    "    plt.barh(topn['feature'], topn['shap_value'])\n",
    "    plt.title(f\"Case {idx} top SHAP contributions (signed)\")\n",
    "    plt.tight_layout()\n",
    "    png_path = os.path.join(CASE_DIR, f\"case_{idx}_shap_local.png\")\n",
    "    plt.savefig(png_path, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Saved SHAP local plot:\", png_path)\n",
    "\n",
    "    # LIME explanation (raw features)\n",
    "    raw_vals = raw_row.values.ravel()\n",
    "    exp = lime_explainer.explain_instance(raw_vals, predict_proba_raw, num_features=10)\n",
    "    lime_list = exp.as_list()  # list of (feature, weight)\n",
    "    lime_df = pd.DataFrame(lime_list, columns=['feature', 'weight'])\n",
    "    lime_csv = os.path.join(CASE_DIR, f\"case_{idx}_lime_local.csv\")\n",
    "    lime_df.to_csv(lime_csv, index=False)\n",
    "    print(\"Saved LIME explanation for case\", idx, \"->\", lime_csv)\n",
    "\n",
    "# 15) Compare SHAP global vs model feature importance (XGBoost built-in)\n",
    "# Get XGBoost feature importance (note: importance keys refer to preprocessed feature indices)\n",
    "# We'll extract XGBoost feature importance on the preprocessed feature names:\n",
    "bst = pipeline.named_steps['clf'].get_booster()\n",
    "# XGBoost feature names default to f0, f1, ... when a numpy matrix is passed.\n",
    "# We'll build a mapping: f{i} -> feature_names[i]\n",
    "xgb_importance = pipeline.named_steps['clf'].get_booster().get_score(importance_type='gain')\n",
    "# Convert to DataFrame\n",
    "xgb_imp_list = []\n",
    "for k,v in xgb_importance.items():\n",
    "    # k like 'f12' -> index 12\n",
    "    idx = int(k.replace('f',''))\n",
    "    fname = feature_names[idx] if idx < len(feature_names) else f\"f{idx}\"\n",
    "    xgb_imp_list.append((fname, v))\n",
    "xgb_imp_df = pd.DataFrame(xgb_imp_list, columns=['feature','gain']).sort_values('gain', ascending=False)\n",
    "XGB_IMP_CSV = \"/mnt/data/xgb_feature_importance.csv\"\n",
    "xgb_imp_df.to_csv(XGB_IMP_CSV, index=False)\n",
    "print(\"Saved XGBoost feature importance to:\", XGB_IMP_CSV)\n",
    "\n",
    "# 16) Save predictions on test set with SHAP top-3 features for each row (approx)\n",
    "# Note: computing SHAP for entire test set may be slow; we'll compute for first 500 rows\n",
    "test_N = min(500, X_test_preprocessed.shape[0])\n",
    "shap_vals_full = explainer.shap_values(X_test_preprocessed[:test_N])\n",
    "top3_list = []\n",
    "for i in range(test_N):\n",
    "    abs_vals = np.abs(shap_vals_full[i])\n",
    "    top3_idx = np.argsort(abs_vals)[-3:][::-1]\n",
    "    top3_features = [feature_names[j] for j in top3_idx]\n",
    "    top3_list.append(\";\".join(top3_features))\n",
    "\n",
    "pred_df = test_df.reset_index(drop=True).iloc[:test_N].copy()\n",
    "pred_df['proba_default'] = pipeline.predict_proba(pred_df)[:,1]\n",
    "pred_df['pred'] = pipeline.predict(pred_df)\n",
    "pred_df['top3_shap'] = top3_list\n",
    "OUT_PRED_CSV = \"/mnt/data/test_predictions_with_top3_shap.csv\"\n",
    "pred_df.to_csv(OUT_PRED_CSV, index=False)\n",
    "print(\"Saved sample test predictions with top3 SHAP to:\", OUT_PRED_CSV)\n",
    "\n",
    "# 17) Summary prints and file paths for deliverables\n",
    "print(\"\\n==== Deliverables (saved) ====\")\n",
    "print(\"Model pipeline:\", MODEL_PATH)\n",
    "print(\"SHAP summary plot:\", SHAP_SUMMARY_PNG)\n",
    "print(\"SHAP feature importance CSV:\", SHAP_CSV)\n",
    "print(\"XGBoost feature importance CSV:\", XGB_IMP_CSV)\n",
    "print(\"Per-case explanations folder:\", CASE_DIR)\n",
    "print(\"Sample test predictions with top3 SHAP:\", OUT_PRED_CSV)\n",
    "\n",
    "print(\"\\nDone. You can download the saved files from the /mnt/data/ directory in the environment.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f265af-f9a8-4100-b788-33309c96d3dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
